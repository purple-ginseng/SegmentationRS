K-Nearest Neighbour (KNN) 是一种简单而有效的监督学习算法,广泛应用于分类和回归任务。它的基本思想是:对于新的输入样本,在特征空间中找到与其最邻近的 K 个训练样本,然后根据这 K 个邻居的标签或值来决定该输入样本的标签或值。

KNN 算法的步骤如下:

1. 准备数据:收集训练数据集,包括样本的特征向量和对应的标签或值。

2. 选择 K 值:确定 K 值,即选择最近邻的数量。K 值的选择会影响模型的偏差和方差。

3. 计算距离:对于新的输入样本,计算其与训练集中每个样本之间的距离。常用的距离度量有欧几里得距离、曼哈顿距离、余弦相似度等。

4. 选择最近邻:选择与输入样本距离最近的 K 个训练样本作为最近邻。

5. 做出预测:对于分类任务,采用多数表决法,即选择 K 个最近邻中出现次数最多的类别作为预测结果;对于回归任务,采用平均法,即计算 K 个最近邻的标签值的平均值作为预测结果。

KNN 算法的优点是:

1. 简单易懂:KNN 算法的原理简单,易于理解和实现。

2. 无需训练:KNN 是一种懒惰学习算法,没有显式的训练过程,只在做出预测时才进行计算。

3. 非参数化:KNN 不对数据的分布做任何假设,适用于各种数据分布。

4. 适用于多分类:KNN 可以直接处理多分类问题,无需进行特殊处理。

但 KNN 算法也有一些缺点:

1. 计算开销大:对于大规模数据集,计算输入样本与所有训练样本之间的距离会很耗时。

2. 存储开销大:KNN 需要存储所有的训练样本,对存储空间的需求较大。

3. 敏感于特征缩放:KNN 使用距离度量,因此对特征的缩放很敏感。需要对特征进行归一化或标准化处理。

4. K 值的选择:K 值的选择会影响模型的性能,需要通过交叉验证等方法进行调优。

在实践中,KNN 算法通常用于以下场景:

1. 小规模数据集:由于 KNN 的计算开销较大,它更适用于小规模数据集。

2. 低维数据:KNN 在低维空间中表现较好,在高维空间中会受到"维度灾难"的影响。

3. 非线性决策边界:KNN 可以逼近任意形状的决策边界,适用于非线性分类问题。

4. 懒惰学习:当训练数据集经常变化,或者需要实时学习和预测时,KNN 是一个不错的选择。

以下是 KNN 算法的 Python 实现示例:

```python
import numpy as np
from collections import Counter

class KNN:
    def __init__(self, k):
        self.k = k
    
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
    
    def predict(self, X):
        y_pred = [self._predict(x) for x in X]
        return np.array(y_pred)
    
    def _predict(self, x):
        # 计算距离
        distances = [np.sqrt(np.sum((x_train - x)**2)) for x_train in self.X_train]
        
        # 获取最近的 k 个索引
        k_indices = np.argsort(distances)[:self.k]
        
        # 获取最近的 k 个标签
        k_nearest_labels = [self.y_train[i] for i in k_indices]
        
        # 多数表决
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]
```

在这个示例中,我们定义了一个 KNN 类,实现了 KNN 算法的训练和预测过程。在 `fit` 方法中,我们简单地存储了训练数据集。在 `predict` 方法中,对于每个输入样本,我们计算其与所有训练样本之间的欧几里得距离,选择最近的 K 个邻居,并通过多数表决法确定预测标签。

需要注意的是,这个示例是一个简化版的实现,没有考虑特征归一化、距离度量的选择、K 值的调优等问题。在实际应用中,还需要根据具体情况进行优化和改进。

总之,K-Nearest Neighbour 是一种简单而强大的监督学习算法,特别适用于小规模、低维、非线性的分类和回归问题。但在使用时,需要注意其计算开销、存储开销、特征缩放等问题,并根据具体任务和数据特点进行适当的调整和优化。